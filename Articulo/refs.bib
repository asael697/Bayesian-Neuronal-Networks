@book{Miggon,
author = {Migon, Helio and Gamerman, Dani and Louzada, Francisco},
  isbn={9781439878804},
  lccn={K13686},
  series={Chapman and Hall CRC Texts in Statistical Science},
year = {2014},
month = {09},
pages = {},
title = {Statistical inference. An integrated approach}
}
@article{Stan,
author = {Stan, Development. Team},
year = {2017a},
title = {Stan: A C++ Library for Probability and Sampling, Version 2.16.0.},
 url = {http://mc-stan.org/.}
}
@article{Stanb,
author = {Stan, Development. Team},
year = {2017b},
title = {Stan: Stan Modeling Language: User’s Guide and Reference Manual.},
 url = {http://mc-stan.org/manual.html.}
}
@article{Paul2017,
   author = {Paul-Christian Bürkner},
   title = {brms: An R Package for Bayesian Multilevel Models Using Stan},
   journal = {Journal of Statistical Software, Articles},
   volume = {80},
   number = {1},
   year = {2017},
   keywords = {Bayesian inference; multilevel model; ordinal data; MCMC; Stan; R},
   abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit  -  among others  -  linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
   issn = {1548-7660},
   pages = {1--28},
   doi = {10.18637/jss.v080.i01},
   url = {https://www.jstatsoft.org/v080/i01}
}
@article{hoffman14a,
  author  = {Matthew D. Hoffman and Andrew Gelman},
  title   = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {1593-1623},
  url     = {http://jmlr.org/papers/v15/hoffman14a.html}
}
@misc{betancourt2017,
    title={A Conceptual Introduction to Hamiltonian Monte Carlo},
    author={Michael Betancourt},
    year={2017},
    eprint={1701.02434},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}
@article{DUANE1987216,
title = {Hybrid Monte Carlo},
journal = {Physics Letters B},
volume = {95},
number = {2},
pages = {216 - 222},
year = {1987},
issn = {0370-2693},
doi = {https://doi.org/10.1016/0370-2693(87)91197-X"},
url = {http://www.sciencedirect.com/science/article/pii/037026938791197X},
author = {Duane, S, et al.}
}
@book{degroot19886,
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {DeGroot, Morris H.},
  biburl = {https://www.bibsonomy.org/bibtex/29166dd4bba0eb5ae538c622033f5f37f/jwbowers},
  citeulike-article-id = {106895},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {0506ce3e89f39df55b02cf75c280c518},
  intrahash = {9166dd4bba0eb5ae538c622033f5f37f},
  keywords = {statistics},
  timestamp = {2009-10-28T04:43:06.000+0100},
  title = {Probability and Statistics},
  year = 1986
}
@article{LKJ2009,
title = "Generating random correlation matrices based on vines and extended onion method",
journal = "Journal of Multivariate Analysis",
volume = "100",
number = "9",
pages = "1989 - 2001",
year = "2009",
issn = "0047-259X",
doi = "https://doi.org/10.1016/j.jmva.2009.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0047259X09000876",
author = "Daniel Lewandowski and Dorota Kurowicka and Harry Joe",
keywords = "Dependence vines, Correlation matrix, Partial correlation, Onion method",
abstract = "We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions."
}
@article{Ramjini,
author = { Ranjini   Natarajan  and  Robert E.   Kass },
title = {Reference Bayesian Methods for Generalized Linear Mixed Models},
journal = {Journal of the American Statistical Association},
volume = {95},
number = {449},
pages = {227-237},
year  = {2000},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2000.10473916},
URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2000.10473916},
eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.2000.10473916}
}
@misc{gronau2017,
    title={A Tutorial on Bridge Sampling},
    author={Quentin F. Gronau and Alexandra Sarafoglou and Dora Matzke and Alexander Ly and Udo Boehm and Maarten Marsman and David S. Leslie and Jonathan J. Forster and Eric-Jan Wagenmakers and Helen Steingroever},
    year={2017},
    eprint={1703.05984},
    archivePrefix={arXiv},
    primaryClass={stat.CO}
}
@article{bayesfactor,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291091},
 abstract = {In a 1935 paper and in his book Theory of probability, Jeffresy developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpies was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P-values, less attention has been given to the Bayes as a practical tool of applied statistics. In this article we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology, and psychology. We emphasize the following points: From Jeffrey's Bayesian viewpoint, the purpose of hypothesis testing is to evaluate the evidence in favor of a scientific theory. Bayes factors offer a way of evaluating evidence in favor of a null hypothesis. Bayes factors provide a way of incorporating external information into the evaluation of evidence about a hypothesis. Bayes factors are very general and do not require alternative models to be nested. Several techniques are available for computing Bayes factors, including asymptotic approximations that are easy to compute using the output from standard packages that maximize likelihoods. In "non-Bayesian significance tests. The Schwarz criterion (or BIC) gives a rough approximation to the logarithm of the Bayes factor, which is easy to use and does not require evaluation of prior distributions. When one is interested in estimation or prediction, Bayes factors may be converted to weights to be attached to various models so that a composite estimate or prediction may be obtained that takes account of structural or model uncertainty. Algorithms have been proposed that allow model uncertainty to be taken into account when the class of models initially considered is very large. Bayes factors are useful for guiding an evolutionary model-building process. It is important, and feasible, to assess the sensitivity of conclusions to the prior distributions used.},
 author = {Robert E. Kass and Adrian E. Raftery},
 journal = {Journal of the American Statistical Association},
 number = {430},
 pages = {773--795},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayes Factors},
 volume = {90},
 year = {1995}
}
@misc{loo,
author = {Vehtari, A. Gelman, A. and Gabry, J.},
 journal ={CRAN},
number = {1.1.0},
 title = {loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models. R Package},
 year = {2017},
url = "https://github.com/jgabry/loo."
}
@article{watanabe,
author = {Sumio Watanabe},
journal = {Journal of Machine Learning Research},
volume ={11},
year  = {2010},
title={Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory},
url = {http://www.jmlr.org/papers/volume11/watanabe10a/watanabe10a.pdf}
}
@article{David,
author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Van Der Linde, Angelika},
title = {Bayesian measures of model complexity and fit},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {64},
number = {4},
pages = {583-639},
keywords = {Bayesian model comparison, Decision theory, Deviance information criterion, Effective number of parameters, Hierarchical models, Information theory, Leverage, Markov chain Monte Carlo methods, Model dimension},
doi = {10.1111/1467-9868.00353},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00353},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00353},
abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the ‘hat’ matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
year = {2002}
}
@misc{vehtari,
    title={Pareto Smoothed Importance Sampling},
    author={Aki Vehtari and Daniel Simpson and Andrew Gelman and Yuling Yao and Jonah Gabry},
    year={2015},
    eprint={1507.02646},
    archivePrefix={arXiv},
    primaryClass={stat.CO}
}
@InProceedings{Christos2017,
  title = 	 {Multiplicative Normalizing Flows for Variational Bayesian Neural Networks},
  author = 	 {Christos Louizos and Max Welling},
  booktitle ={Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2218--2227},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/louizos17a/louizos17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/louizos17a.html},
  abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.}
}
@inproceedings{Bhat2006,
  title={Bayesian Neuroanl Networks},
  author={Pushpalatha C. Bhat and Harrison B. Prosper},
  url ={https://www.semanticscholar.org/paper/BAYESIAN-NEURAL-NETWORKS-Bhat-Prosper},
  doi = {10.1142/9781860948985_0032},
  year={2006},
abstract = 	 {The training of neural networks can be viewed as a problem of inference, which can be addressed from a Bayesian
viewpoint. This perspective leads to a method, new to the field of particle physics, called Bayesian neural networks
(BNN). After a brief overview of the method we illustrate how it can be usefully deployed in particle physics research.}
}
@article{Paige2001,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2673435},
 abstract = {Approximate marginal Bayesian computation and inference are developed for neural network models. The marginal considerations include determination of approximate Bayes factors for model choice about the number of nonlinear sigmoid terms, approximate predictive density computation for a future observable and determination of approximate Bayes estimates for the nonlinear regression function. Standard conjugate analysis applied to the linear parameters leads to an explicit posterior on the nonlinear parameters. Further marginalisation is performed using Laplace approximations. The choice of prior and the use of an alternative sigmoid lead to posterior invariance in the nonlinear parameter which is discussed in connection with the lack of sigmoid identifiability. A principal finding is that parsimonious model choice is best determined from the list of modal estimates used in the Laplace approximation of the Bayes factors for various numbers of sigmoids. By comparison, the values of the various Bayes factors are of only secondary importance. The proposed methods are illustrated in the context of two nonlinear datasets that involve respectively univariate and multivariate nonlinear regression models.},
 author = {R. L. Paige and R. W. Butler},
 journal = {Biometrika},
 number = {3},
 pages = {623--641},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Bayesian Inference in Neural Networks},
 volume = {88},
 year = {2001}
}
@inproceedings{Arya2017,
  title={Improving the Identifiability of Neural Networks for Bayesian Inference},
  author={Arya A. Pourzanjani and Richard M. Jiang and Linda R. Petzold},
  url ={https://www.semanticscholar.org/paper/Improving-the-Identifiability-of-Neural-Networks-Pourzanjani-Jiang},
  doi = {46932278},
  year={2017},
  abstract ={Accurate inference of the parameters in the highly complex and multi-modal likelihoods of Neural
Networks(NNs) is incredibly difficult for any algorithm. In part, this challenge is caused by the
significant over-parameterization of the model, resulting in many equivalent solutions and thus a
model unidentifiability problem. In this paper, we explore the unidentifiability problem for NNs as it
manifests in two ways: arbitrary permutations of the hidden nodes, which we denote as weight-space
symmetry, and arbitrary scaling under rectified linear-unit (ReLU) nonlinearites, which we denote
as scaling symmetry. We show how these unidentifiabilities pose issues for both Markov Chain
Monte Carlo (MCMC) and Variational Inference (VI). Finally, we introduce two reparameterizations
of the model in the form of parameter constraints and prove that they resolve the aforementioned
unidentifiability issues, showing some experiments and offering implementations in the form of
coordinate transforms..}
}
@article{Cheng2018,
  author    = {Xi Cheng and
               Bohdan Khomtchouk and
               Norman S. Matloff and
               Pete Mohanty},
  title     = {Polynomial Regression As an Alternative to Neural Nets},
  journal   = {CoRR},
  volume    = {abs/1806.06850},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.06850},
  archivePrefix = {arXiv},
  eprint    = {1806.06850},
  timestamp = {Tue, 03 Mar 2020 16:02:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-06850.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Williams1996,
          author={Christopher K. I. Williams},
          title={Computing with Infinite Networks},
          year={1996},
          cdate={820454400000},
          pages={295-301},
          url={http://papers.nips.cc/paper/1197-computing-with-infinite-networks},
          booktitle={NIPS},
          crossref={conf/nips/1996N}
}
@article{Silverman1985,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2345542},
 abstract = {Non-parametric regression using cubic splines is an attractive, flexible and widely-applicable approach to curve estimation. Although the basic idea was formulated many years ago, the method is not as widely known or adopted as perhaps it should be. The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology. Particular subjects covered include the basic principles of the method; the relation with moving average and other smoothing methods; the automatic choice of the amount of smoothing; and the use of residuals for diagnostic checking and model adaptation. The question of providing inference regions for curves-and for relevant properties of curves--is approached via a finite-dimensional Bayesian formulation.},
 author = {B. W. Silverman},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--52},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting},
 volume = {47},
 year = {1985}
}
@article{Rigby2005,
author = {Rigby, R. A. and Stasinopoulos, D. M.},
title = {Generalized additive models for location, scale and shape},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
volume = {54},
number = {3},
pages = {507-554},
keywords = {Beta–binomial distribution, Box–Cox transformation, Centile estimation, Cubic smoothing splines, Generalized linear mixed model, LMS method, Negative binomial distribution, Non-normality, Nonparametric models, Overdispersion, Penalized likelihood, Random effects, Skewness and kurtosis},
doi = {10.1111/j.1467-9876.2005.00510.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00510.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9876.2005.00510.x},
abstract = {Summary.  A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton–Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
year = {2005}
}
@book{gramacy2020surrogates,
  title = {Surrogates: {G}aussian Process Modeling, Design and \
    Optimization for the Applied Sciences},
  author = {Robert B. Gramacy},
  publisher = {Chapman Hall/CRC},
  address = {Boca Raton, Florida},
  note = {\url{http://bobby.gramacy.com/surrogates/}},
  year = {2020}
}
@book{haykin2009,
  abstract = {Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists. Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/ Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.},
  added-at = {2017-03-18T17:31:57.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Haykin, Simon S.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5015812328aaeccd73d8b03a7e36831/vngudivada},
  edition = {Third},
  interhash = {4cef19efafc52ae42607f9832a205214},
  intrahash = {e5015812328aaeccd73d8b03a7e36831},
  keywords = {Book Learning NeuralNetwork},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks and learning machines},
  year = 2009
}
@misc{tensorflow2015,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and  Zhifeng~Chen and   Craig~Citro and  Greg~S.~Corrado and  Andy~Davis   and Jeffrey~Dean and  Matthieu~Devin and  Sanjay~Ghemawat and  Ian~Goodfellow and Andrew~Harp and  Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and  Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and  Chris~Olah and  Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and  Paul~Tucker and  Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  year={2015},
}

